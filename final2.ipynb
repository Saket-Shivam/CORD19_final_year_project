{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langdetect\n",
    "#!pip install tqdm\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#!pip install ipywidgets\n",
    "#!jupyter nbextension enable --py widgetsnbextension\n",
    "\n",
    "#!pip install dash\n",
    "#!pip install dash-renderer\n",
    "#!pip install dash_html_components\n",
    "#!pip install dash_core_components\n",
    "#!pip install jupyter_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from langdetect import DetectorFactory, detect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "import io\n",
    "import string\n",
    "import plotly.express as px\n",
    "import unicodedata\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, IntText, widgets, FileUpload\n",
    "import nltk\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Plotly dash Imports\n",
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import jupyter_dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:8050/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:05] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:07] \"\u001b[37mGET /_dash-layout HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:07] \"\u001b[37mGET /_dash-dependencies HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:07] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:22] \"\u001b[37mGET /_dash-component-suites/dash_table/async-highlight.js HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:22] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 204 -\n",
      "127.0.0.1 - - [22/Jun/2021 19:58:22] \"\u001b[37mGET /_dash-component-suites/dash_table/async-table.js HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a920ca603740598339f609e5651fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=409.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 409\n",
      "\n",
      "{'en': 405, 'es': 2, 'fr': 2}\n",
      "    values  First keywords middle keywords  last keywoprds\n",
      "0       52       ('severe'   'respiratory'     'syndrome')\n",
      "1       52  ('coronavirus'       'disease'      'covid19')\n",
      "2       41  ('respiratory'      'syndrome'  'coronavirus')\n",
      "3       12      ('disease'       'covid19'     'pandemic')\n",
      "4        8        ('novel'   'coronavirus'      'disease')\n",
      "5        8     ('distress'      'syndrome'         'ards')\n",
      "6        8  ('respiratory'      'distress'     'syndrome')\n",
      "7        8       ('caused'        'severe'  'respiratory')\n",
      "8        7      ('covid19'       'mhealth'         'tool')\n",
      "9        7  ('respiratory'         'tract'    'infection')\n",
      "10       6     ('adjusted'          'odds'        'ratio')\n",
      "11       6        ('basic'  'reproduction'       'number')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jun/2021 19:58:59] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c78f21d3d94dd89be3567d8d6b64ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 521\n",
      "\n",
      "{'af': 15,\n",
      " 'ca': 17,\n",
      " 'cs': 17,\n",
      " 'cy': 69,\n",
      " 'da': 16,\n",
      " 'de': 4,\n",
      " 'en': 11,\n",
      " 'es': 4,\n",
      " 'et': 5,\n",
      " 'fi': 8,\n",
      " 'fr': 10,\n",
      " 'hr': 22,\n",
      " 'hu': 18,\n",
      " 'id': 3,\n",
      " 'it': 5,\n",
      " 'lt': 5,\n",
      " 'lv': 10,\n",
      " 'nl': 9,\n",
      " 'no': 5,\n",
      " 'pl': 61,\n",
      " 'pt': 13,\n",
      " 'ro': 10,\n",
      " 'sk': 25,\n",
      " 'sl': 22,\n",
      " 'so': 60,\n",
      " 'sq': 35,\n",
      " 'sv': 9,\n",
      " 'sw': 12,\n",
      " 'tl': 9,\n",
      " 'tr': 3,\n",
      " 'vi': 9}\n",
      "   values First keywords middle keywords last keywoprds\n",
      "0       1    ('8otl3781'      'h9asb4nt'    'opf1qwgl')\n",
      "1       1    ('opf1qwgl'      'bsgucexp'    '7id8tonh')\n",
      "2       1    ('vif0h7f9'      'vfoxp8tl'    '8otl3781')\n",
      "3       1    ('qs83or5w'      '1o9oqnd6'    'tded20ih')\n",
      "4       1    ('h9asb4nt'      'opf1qwgl'    'bsgucexp')\n",
      "5       1    ('ic3tdx59'      'qs83or5w'    '1o9oqnd6')\n",
      "6       1    ('tded20ih'      'vif0h7f9'    'vfoxp8tl')\n",
      "7       1    ('vfoxp8tl'      '8otl3781'    'h9asb4nt')\n",
      "8       1    ('1o9oqnd6'      'tded20ih'    'vif0h7f9')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jun/2021 19:59:29] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3ee3e679e948c89ec34dc13d4ec28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total: 520\n",
      "\n",
      "{'af': 1, 'ca': 1, 'de': 2, 'en': 504, 'es': 5, 'fr': 5, 'ro': 1, 'sv': 1}\n",
      "    values  First keywords middle keywords   last keywoprds\n",
      "0        7       ('severe'   'respiratory'      'syndrome')\n",
      "1        4       ('impact'       'covid19'      'pandemic')\n",
      "2        4  ('coronavirus'       'disease'       'covid19')\n",
      "3        4  ('respiratory'      'syndrome'   'coronavirus')\n",
      "4        3  ('angiotensin'    'converting'        'enzyme')\n",
      "5        3  ('respiratory'     'syncytial'         'virus')\n",
      "6        3   ('infectious'    'bronchitis'         'virus')\n",
      "7        3  ('coronavirus'       'disease'      'pandemic')\n",
      "8        3   ('systematic'        'review'  'metaanalysis')\n",
      "9        2      ('covid19'        'social'    'distancing')\n",
      "10       2         ('amid'       'covid19'      'pandemic')\n",
      "11       2      ('therapy'    'critically'           'ill')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Jun/2021 19:59:47] \"\u001b[37mPOST /_dash-update-component HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import datetime\n",
    "import io\n",
    "import jupyter_dash\n",
    "\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash import no_update\n",
    "import dash\n",
    "from dash.dependencies import Input, Output, State\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "import dash_table\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "external_stylesheets = ['https://codepen.io/chriddyp/pen/bWLwgP.css']\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=external_stylesheets)\n",
    "\n",
    "app.layout = html.Div([\n",
    "    dcc.Upload(\n",
    "        id='upload-data',\n",
    "        children=html.Div([\n",
    "            'Drag and Drop or ',\n",
    "            html.A('Select Files')\n",
    "        ]),\n",
    "        style={\n",
    "            'width': '100%',\n",
    "            'height': '60px',\n",
    "            'lineHeight': '60px',\n",
    "            'borderWidth': '1px',\n",
    "            'borderStyle': 'dashed',\n",
    "            'borderRadius': '5px',\n",
    "            'textAlign': 'center',\n",
    "            'margin': '10px'\n",
    "        },\n",
    "        # Allow multiple files to be uploaded\n",
    "        multiple=True\n",
    "    ),\n",
    "    html.Div(id='output-div'),\n",
    "    html.Div(id='output-data-upload'),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "def parse_contents(contents, filename, date):\n",
    "    content_type, content_string = contents.split(',')\n",
    "\n",
    "    decoded = base64.b64decode(content_string)\n",
    "    try:\n",
    "        if 'csv' in filename:\n",
    "            # Assume that the user uploaded a CSV file\n",
    "            df = pd.read_csv(\n",
    "                io.StringIO(decoded.decode('utf-8')))\n",
    "        elif 'xls' in filename:\n",
    "            # Assume that the user uploaded an excel file\n",
    "            df = pd.read_excel(io.BytesIO(decoded))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return html.Div([\n",
    "            'There was an error processing this file.'\n",
    "        ])\n",
    "\n",
    "    return html.Div([\n",
    "        html.H5(filename),\n",
    "        html.H6(datetime.datetime.fromtimestamp(date)),\n",
    "        html.P(\"Select Column\"),\n",
    "        dcc.Dropdown(id='column-data',\n",
    "                     options=[{'label':x, 'value':x} for x in df.columns]),\n",
    "        html.Button(id=\"submit-button\", children=\"Create Graph\"),\n",
    "        \n",
    "        html.Hr(),\n",
    "\n",
    "        dash_table.DataTable(\n",
    "            data=df.to_dict('records'),\n",
    "            columns=[{'name': i, 'id': i} for i in df.columns],\n",
    "            page_size=15\n",
    "        ),\n",
    "        dcc.Store(id='stored-data', data=df.to_dict('records')),\n",
    "\n",
    "        html.Hr(),  # horizontal line\n",
    "\n",
    "        # For debugging, display the raw contents provided by the web browser\n",
    "        html.Div('Raw Content'),\n",
    "        html.Pre(contents[0:200] + '...', style={\n",
    "            'whiteSpace': 'pre-wrap',\n",
    "            'wordBreak': 'break-all'\n",
    "        })\n",
    "    ])\n",
    "\n",
    "@app.callback(Output('output-data-upload', 'children'),\n",
    "              Input('upload-data', 'contents'),\n",
    "              State('upload-data', 'filename'),\n",
    "              State('upload-data', 'last_modified'))\n",
    "def update_output(list_of_contents, list_of_names, list_of_dates):\n",
    "    if list_of_contents is not None:\n",
    "        children = [\n",
    "            parse_contents(c, n, d) for c, n, d in\n",
    "            zip(list_of_contents, list_of_names, list_of_dates)]\n",
    "        return children\n",
    "\n",
    "\n",
    "@app.callback(Output('output-div', 'children'),\n",
    "              Input('submit-button','n_clicks'),\n",
    "              State('stored-data','data'),\n",
    "              State('column-data','value'))\n",
    "def make_graphs(n, data, key_data):\n",
    "    if n is None:\n",
    "        return no_update\n",
    "    else:\n",
    "        #\n",
    "            df_covid = pd.DataFrame(columns=['abstract'])\n",
    "            data2 = pd.DataFrame(data)\n",
    "            df_covid['abstract'] = data2[key_data]\n",
    "            #df_covid['abstract'] = data2.abstract\n",
    "            \n",
    "            #df_covid\n",
    "            df_covid.drop_duplicates(['abstract'], inplace=True)\n",
    "            df_covid.dropna(inplace=True)\n",
    "            #df_covid.info()\n",
    "            #Dropping non-English articles\n",
    "\n",
    "            DetectorFactory.seed = 0\n",
    "\n",
    "            # hold label - language\n",
    "            languages = []\n",
    "\n",
    "            # go through each text\n",
    "            for ii in tqdm(range(0,len(df_covid))):\n",
    "                # split by space into list, take the first x intex, join with space\n",
    "                text = df_covid.iloc[ii]['abstract'].split(\" \")\n",
    "\n",
    "                lang = \"en\"\n",
    "                try:\n",
    "                    if len(text) > 50:\n",
    "                        lang = detect(\" \".join(text[:50]))\n",
    "                    elif len(text) > 0:\n",
    "                        lang = detect(\" \".join(text[:len(text)]))\n",
    "                # ught... beginning of the document was not in a good format\n",
    "                except Exception as e:\n",
    "                    all_words = set(text)\n",
    "                    try:\n",
    "                        lang = detect(\" \".join(all_words))\n",
    "\n",
    "                    except Exception as e:        \n",
    "                        lang = \"unknown\"\n",
    "                        pass\n",
    "\n",
    "                # get the language    \n",
    "                languages.append(lang)\n",
    "            # Number of articles in each language\n",
    "\n",
    "            languages_dict = {}\n",
    "            for lang in set(languages):\n",
    "                languages_dict[lang] = languages.count(lang)\n",
    "\n",
    "            print(\"Total: {}\\n\".format(len(languages)))\n",
    "            pprint(languages_dict)\n",
    "            # Dropping non english articles\n",
    "\n",
    "            df_covid['language'] = languages\n",
    "            df_covid = df_covid[df_covid['language'] == 'en'] \n",
    "            #df_covid.info()\n",
    "            df_covid = df_covid.drop(['language'], axis = 1) \n",
    "            #df_covid.head()\n",
    "            df_covid.to_csv('modified.csv')\n",
    "            \n",
    "            dataset = pd.read_csv('modified.csv')\n",
    "            #dataset.head()\n",
    "            dataset.columns = ['label','abstract']\n",
    "            #dataset.head()\n",
    "            dataset = dataset.drop(columns = 'label')\n",
    "\n",
    "            def remove_punctuation(txt):\n",
    "                txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "                return txt_nopunct\n",
    "\n",
    "            dataset['abstract_clean'] = dataset['abstract'].apply(lambda x: remove_punctuation(x))\n",
    "            #dataset.head()\n",
    "            \n",
    "            import re\n",
    "            def tokenize(txt):\n",
    "                tokens = re.split('\\W+', txt)\n",
    "                return tokens\n",
    "\n",
    "            dataset['abstract_clean_tokenized'] = dataset['abstract_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "            #dataset.head()\n",
    "\n",
    "            dataset = dataset.drop(columns = ['abstract'])\n",
    "            dataset = dataset.drop(columns = ['abstract_clean'])\n",
    "            #dataset.head()\n",
    "            def lemmatization(token_text):\n",
    "                text = [wn.lemmatize(word) for word in token_text]\n",
    "                return text\n",
    "\n",
    "            dataset['abs_cln_lemm'] = dataset['abstract_clean_tokenized'].apply(lambda x : lemmatization(x))\n",
    "            dataset.head()\n",
    "\n",
    "            #Remove stop words\n",
    "            import nltk\n",
    "            stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "            def remove_stopwords(txt_tokenized):\n",
    "                txt_clean = [word for word in txt_tokenized if word not in stopwords]\n",
    "                return txt_clean\n",
    "\n",
    "            dataset['abs_no_sw'] = dataset['abs_cln_lemm'].apply(lambda x: remove_stopwords(x))\n",
    "            #dataset.head()\n",
    "\n",
    "            dataset = dataset.drop(columns = ['abstract_clean_tokenized'])\n",
    "            dataset = dataset.drop(columns = ['abs_cln_lemm'])\n",
    "            #dataset.head()\n",
    "\n",
    "            dataset.to_csv('modified_cleaned.csv')\n",
    "            # Custom word removal\n",
    "            new_words = ('2019','95','ci','patient', 'mental', 'health', 'risk', 'factor', 'formula', 'see', 'text', 'polymerase', 'chain', 'reaction' ,'supplementary',\n",
    "                         'material', 'available' ,'intensive', 'care', 'unit', 'contains', 'supplementary', \n",
    "                        'ha', 'acute', 'study', 'wa', '2', '19', 'white', 'blood', 'cell', 'middle', 'east',\n",
    "                         'aim', 'sars', 'cov', 'electronic', 'online', 'version', 'article',\n",
    "                         'access', 'service', 'among', 'waste', 'collector', 'routine', 'treatment', 'combined', 'chinese', 'herbal', \n",
    "                        'liaison', 'sarscov2', 's1s2', 'igg', 'assay', 'mechanical', 'ventilation', 's1', 'n', 'p', '0001')\n",
    "\n",
    "            def custom_word_removal(txt_tokenized):\n",
    "                txt_clean = [word for word in txt_tokenized if word not in new_words]\n",
    "                return txt_clean\n",
    "\n",
    "            dataset['abstract_keywords'] = dataset['abs_no_sw'].apply(lambda x: custom_word_removal(x))\n",
    "            #dataset.head()\n",
    "\n",
    "            dataset = dataset.drop(columns = ['abs_no_sw'])\n",
    "            #dataset.head()\n",
    "\n",
    "            dataset.to_csv('abstract_keywords.csv')\n",
    "            df_ngram = pd.read_csv('abstract_keywords.csv', usecols=[1])\n",
    "            \n",
    "            def basic_clean(text):\n",
    "\n",
    "              text = (unicodedata.normalize('NFKD', text)\n",
    "                .encode('ascii', 'ignore')\n",
    "                .decode('utf-8', 'ignore')\n",
    "                .lower())\n",
    "              words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "              return words\n",
    "            words = basic_clean(''.join(str(df_ngram['abstract_keywords'].tolist())))\n",
    "            bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:12]\n",
    "            bigrams_series.to_csv('ram.csv')\n",
    "            bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))\n",
    "            bigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:12]\n",
    "            bigrams_series.to_csv('tram.csv')\n",
    "            data1=pd.read_csv('tram.csv')\n",
    "            data1.columns=['keywords','values']\n",
    "            data1.dropna(inplace = True)\n",
    "            new = data1[\"keywords\"].str.split(\", \", n = 2, expand = True)\n",
    "            data1[\"First keywords\"]= new[0]\n",
    "            data1[\"middle keywords\"]= new[1]\n",
    "            data1[\"last keywoprds\"]= new[2]\n",
    "            data1.drop(columns =[\"keywords\"], inplace = True)\n",
    "            print (data1)\n",
    "            data1.to_csv('new.csv')\n",
    "            s = open('new.csv','r').read()\n",
    "\n",
    "            chars = ('(',')') # etc\n",
    "            for c in chars:\n",
    "              s = ''.join( s.split(c) )\n",
    "\n",
    "            out_file = open('new.csv','w')\n",
    "            out_file.write(s)\n",
    "            out_file.close()\n",
    "            data3=pd.read_csv('new.csv')\n",
    "            sunburst_fig=px.sunburst(data3,path=['First keywords','middle keywords','last keywoprds'],values=\"values\")\n",
    "            #sunburst_fig = px.bar(data, x=key_data, y=y_data)\n",
    "            # print(data)\n",
    "            return dcc.Graph(figure=sunburst_fig)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#meta_df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "#meta_df.head()\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#uploade any file from system \n",
    "#import ipywidgets as widgets\n",
    "##import ipython_blocking\n",
    "#uploader=widgets.FileUpload(\n",
    "#    accept='*.csv',\n",
    "#    multiple=False\n",
    "#)\n",
    "#display(uploader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %block uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#[input_file]=uploader.value\n",
    "#meta_df=pd.read_csv(input_file,low_memory=False)\n",
    "#meta_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_df = pd.read_csv('metadata.csv', low_memory=False)\n",
    "#meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#output=widgets.Output()\n",
    "#ALL='ALL'\n",
    "#dropdown_rows=widgets.Dropdown(options=pd.read_csv(input_file,nrows=0))\n",
    "#display(dropdown_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %block dropdown_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid = pd.DataFrame(columns=['abstract'])\n",
    "#df_covid['abstract'] = df.abstract\n",
    "#df_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid.drop_duplicates(['abstract'], inplace=True)\n",
    "#df_covid.dropna(inplace=True)\n",
    "'''df_covid.info()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langdetect import DetectorFactory, detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping non-English articles\n",
    "\n",
    "'''DetectorFactory.seed = 0\n",
    "\n",
    "# hold label - language\n",
    "languages = []\n",
    "\n",
    "# go through each text\n",
    "for ii in tqdm(range(0,len(df_covid))):\n",
    "    # split by space into list, take the first x intex, join with space\n",
    "    text = df_covid.iloc[ii]['abstract'].split(\" \")\n",
    "    \n",
    "    lang = \"en\"\n",
    "    try:\n",
    "        if len(text) > 50:\n",
    "            lang = detect(\" \".join(text[:50]))\n",
    "        elif len(text) > 0:\n",
    "            lang = detect(\" \".join(text[:len(text)]))\n",
    "    # ught... beginning of the document was not in a good format\n",
    "    except Exception as e:\n",
    "        all_words = set(text)\n",
    "        try:\n",
    "            lang = detect(\" \".join(all_words))\n",
    "        \n",
    "        except Exception as e:        \n",
    "            lang = \"unknown\"\n",
    "            pass\n",
    "    \n",
    "    # get the language    \n",
    "    languages.append(lang)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of articles in each language\n",
    "\n",
    "'''languages_dict = {}\n",
    "for lang in set(languages):\n",
    "    languages_dict[lang] = languages.count(lang)\n",
    "    \n",
    "print(\"Total: {}\\n\".format(len(languages)))\n",
    "pprint(languages_dict)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping non english articles\n",
    "\n",
    "'''df_covid['language'] = languages\n",
    "df_covid = df_covid[df_covid['language'] == 'en'] \n",
    "df_covid.info()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_covid = df_covid.drop(['language'], axis = 1) \n",
    "df_covid.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid.to_csv('modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_covid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_csv('modified.csv')\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.columns = ['label', 'abstract']\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = dataset.drop(columns = 'label')\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def remove_punctuation(txt):\n",
    "    txt_nopunct = \"\".join([c for c in txt if c not in string.punctuation])\n",
    "    return txt_nopunct'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset['abstract_clean'] = dataset['abstract'].apply(lambda x: remove_punctuation(x))\n",
    "#dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import re\n",
    "def tokenize(txt):\n",
    "    tokens = re.split('\\W+', txt)\n",
    "    return tokens'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dataset['abstract_clean_tokenized'] = dataset['abstract_clean'].apply(lambda x: tokenize(x.lower()))\n",
    "dataset.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dataset = dataset.drop(columns = ['abstract'])\n",
    "dataset = dataset.drop(columns = ['abstract_clean'])\n",
    "dataset.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Lemmatization\n",
    "def lemmatization(token_text):\n",
    "    text = [wn.lemmatize(word) for word in token_text]\n",
    "    return text'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "'''dataset['abs_cln_lemm'] = dataset['abstract_clean_tokenized'].apply(lambda x : lemmatization(x))\n",
    "dataset.head()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def remove_stopwords(txt_tokenized):\n",
    "    txt_clean = [word for word in txt_tokenized if word not in stopwords]\n",
    "    return txt_clean\n",
    "\n",
    "#Remove stop words\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "dataset['abs_no_sw'] = dataset['abs_cln_lemm'].apply(lambda x: remove_stopwords(x))\n",
    "dataset.head()\n",
    "\n",
    "dataset = dataset.drop(columns = ['abstract_clean_tokenized'])\n",
    "dataset = dataset.drop(columns = ['abs_cln_lemm'])\n",
    "dataset.head()\n",
    "\n",
    "dataset.to_csv('modified_cleaned.csv')\n",
    "\n",
    "# Custom word removal\n",
    "new_words = ('2019','95','ci','patient', 'mental', 'health', 'risk', 'factor', 'formula', 'see', 'text', 'polymerase', 'chain', 'reaction' ,'supplementary',\n",
    "             'material', 'available' ,'intensive', 'care', 'unit', 'contains', 'supplementary', \n",
    "            'ha', 'acute', 'study', 'wa', '2', '19', 'white', 'blood', 'cell', 'middle', 'east',\n",
    "             'aim', 'sars', 'cov', 'electronic', 'online', 'version', 'article',\n",
    "             'access', 'service', 'among', 'waste', 'collector', 'routine', 'treatment', 'combined', 'chinese', 'herbal', \n",
    "            'liaison', 'sarscov2', 's1s2', 'igg', 'assay', 'mechanical', 'ventilation', 's1', 'n', 'p', '0001')\n",
    "\n",
    "def custom_word_removal(txt_tokenized):\n",
    "    txt_clean = [word for word in txt_tokenized if word not in new_words]\n",
    "    return txt_clean\n",
    "\n",
    "dataset['abstract_keywords'] = dataset['abs_no_sw'].apply(lambda x: custom_word_removal(x))\n",
    "dataset.head()\n",
    "\n",
    "dataset = dataset.drop(columns = ['abs_no_sw'])\n",
    "dataset.head()\n",
    "\n",
    "dataset.to_csv('abstract_keywords.csv')\n",
    "\n",
    "df_ngram = pd.read_csv('abstract_keywords.csv', usecols=[1])\n",
    "\n",
    "df_ngram\n",
    "\n",
    "def basic_clean(text):\n",
    "\n",
    "  text = (unicodedata.normalize('NFKD', text)\n",
    "    .encode('ascii', 'ignore')\n",
    "    .decode('utf-8', 'ignore')\n",
    "    .lower())\n",
    "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "  return words\n",
    "\n",
    "words = basic_clean(''.join(str(df_ngram['abstract_keywords'].tolist())))\n",
    "\n",
    "words[:20]\n",
    "\n",
    "(pd.Series(nltk.ngrams(words, 2)).value_counts())[:12]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 3)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 4)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 5)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 6)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 7)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(pd.Series(nltk.ngrams(words, 8)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams_series = (pd.Series(nltk.ngrams(words, 2)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams_series.to_csv('ram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigrams_series.sort_values().plot.barh(color='blue', width=.9, figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigrams_series = (pd.Series(nltk.ngrams(words, 3)).value_counts())[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trigrams_series.to_csv('tram.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''data=pd.read_csv('tram.csv')\n",
    "data.columns=['keywords','values']\n",
    "data.dropna(inplace = True)\n",
    "new= data[\"keywords\"].str.split(\", \", n = 2, expand = True)\n",
    "data[\"First_keywords\"]= new[0]\n",
    "data[\"middle_keywords\"]= new[1]\n",
    "data[\"last_keywoprds\"]= new[2]\n",
    "data.drop(columns =[\"keywords\"], inplace = True)\n",
    "print (data)\n",
    "data.to_csv('new.csv')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''df_data=pd.read_csv('new.csv')\n",
    "plot_output = widgets.Output()\n",
    "output_first = widgets.Output()\n",
    "output=widgets.Output()\n",
    "\n",
    "ALL='ALL'\n",
    "def colour_ge_value(value, comparison):\n",
    "    if value >= comparison:\n",
    "        return 'color: red'\n",
    "    else:\n",
    "        return 'color: black'\n",
    "def unique_sorted_values_plus_ALL(array):\n",
    "    unique = array.unique().tolist()\n",
    "    unique.sort()\n",
    "    unique.insert(0, ALL)\n",
    "    return unique\n",
    "dropdown_first = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_data.First_keywords))\n",
    "#dropdown_middle = widgets.Dropdown(options = unique_sorted_values_plus_ALL(df_data.middle_keywords))\n",
    "bounded_num = widgets.BoundedFloatText(min=0, max=100000, value=5, step=1, description='Number:')\n",
    "\n",
    "def common_filtering(first, num):\n",
    "    output.clear_output()\n",
    "    plot_output.clear_output()\n",
    "    if(first == ALL):\n",
    "        common_filter = df_data\n",
    "    \n",
    "    else:\n",
    "        common_filter = df_data[(df_data.First_keywords == first)]\n",
    "    with output:\n",
    "        display(common_filter.style.applymap(lambda x: colour_ge_value(x, num), subset=['values']))\n",
    "    \n",
    "    common_filter.to_csv(\"updated.csv\")\n",
    "    df_new=pd.read_csv('updated.csv')\n",
    "    \n",
    "    with plot_output:\n",
    "        fig=px.sunburst(df_new,path=['First_keywords','middle_keywords','last_keywoprds'],values=\"values\")\n",
    "        fig.show()\n",
    "        #sns.kdeplot(common_filter['values'], shade=True)\n",
    "        #plt.show()\n",
    "    \n",
    "    #fig=px.sunburst(df_new,path=['First_keywords','middle_keywords','last_keywoprds'],values=\"values\")\n",
    "    #fig.show()\n",
    "    \n",
    "def dropdown_first_eventhandler(change):\n",
    "    common_filtering(change.new,bounded_num.value)\n",
    "\n",
    "#def dropdown_middle_eventhandler(change):\n",
    "#    common_filtering(dropdown_first.value, change.new,bounded_num.value)\n",
    "    \n",
    "def bounded_num_eventhandler(change):\n",
    "    common_filtering(dropdown_first.value,change.new)\n",
    "        \n",
    "dropdown_first.observe(dropdown_first_eventhandler, names='value')\n",
    "#dropdown_middle.observe(dropdown_middle_eventhandler, names='value')\n",
    "bounded_num.observe(bounded_num_eventhandler, names='value')\n",
    "\n",
    "display(dropdown_first)\n",
    "#display(dropdown_middle)\n",
    "display(bounded_num)\n",
    "\n",
    "display(output)\n",
    "display(plot_output)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig=px.sunburst(df_new,path=['First keywords','middle keywords','last keywoprds'],values=\"values\")\n",
    "#fig.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
